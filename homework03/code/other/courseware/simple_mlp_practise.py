# -*- coding: utf-8 -*-
"""Simple_MLP_Practise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/anniewit/IANNWTF-2020/blob/main/Simple_MLP_Practise.ipynb
"""

import numpy as np
# NEXT LINE ONLY FOR COLAB!
# %tensorflow_version 2.x
import tensorflow as tf
import matplotlib.pyplot as plt
# COMMENT OUT THIS LINE FOR COLAB!
# %matplotlib notebook

print(tf.__version__)

"""**Disclaimer:** This Notebook heavily depends on the work of Luke Effenberger who held the course last year. You can find his original material under:
https://lukeeffenberger.github.io/IANNWTF-2019/

### Create a simple regression dataset
"""

training_data_xs = np.linspace(-5,5, 20, dtype=np.float32)   ### TODO We need more data so make 200 points instead of 20 out of it 
def f(x):
    return 0.02*(x**3-x**2+2*x)+0.3         # FYI we adapted the function slightly compared to the other notebook
training_data_ys = np.array([f(x) for x in training_data_xs], dtype=np.float32)

### TODO Make some test data points
# They should be on the same line as the train data, but it should ideally be examples that are not included in the training set
# you could sample some from the data using np.random.choice()



plt.scatter(training_data_xs, training_data_ys, c='red')
plt.xlim(-5,5)

# First understand the shape that your data has.
print(training_data_xs.shape)
print(training_data_ys.shape)

### TODO Do the same for the test data set

"""### Convert it to a tf Dataset"""

# It is easiest to use this dataset with TensorFlow by converting
# it to a tf.data.Dataset (https://www.tensorflow.org/api_docs/python/tf/data/Dataset).
# If, as in our case, you want to convert Numpy Arrays into a dataset
# use tf.data.Dataset.from_tensor_slices
# reshape the data to convert (1,) into (1,1)
# Shuffe the data. If you want, you can also batch your data (e.g batchsize = 20)

"""### Build a simple network.

The last time we have used the class Linear. This time we want to use already inbuilt layers from **keras!** 
- Use 2 hidden layers with 256 neurons each and relu activation function. 
- The output layer has 1 neuron and no activation function.
"""

# We can stack multiple layers onto another to build a deep
# neural network. Let' start with a simple MLP with one
# hidden layer.

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer


# Formally we define another layer. So we have to inherit again.
class MLP(Model):
    
    def __init__(self):
        # TODO call the super init again.

       # TODO instantiate the layers that our network has.
  
        
    # In the call function we define the forward pass of the network.
    def call(self, x):
        # TODO 
        return # TODO

"""### Define Training and Testing Steps."""

def train_step(model, input, target, loss_function, optimizer):
    # compute and apply gradients using gradient Tape
     
    return loss 


def test(model, test_data, loss_function):
    # test over complete test data
    test_loss_aggregator = []

    for (input, target) in test_data:
        # get a prediction from the model
        
        # calcuate the loss

        # append the loss to the list (you might need to make an array out of it)

    test_loss = np.mean(test_loss_aggregator)

    return test_loss

tf.keras.backend.clear_session()

### Hyperparameters
num_epochs = 250
learning_rate = 0.01
running_average_factor = 0.95

# TODO Initialize the model.
model = 

# TODO Initialize the loss: Mean Squared Error. Check out 'tf.keras.losses'.
mse = 

# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'
optimizer = 

# Initialize lists for later visualization.
train_losses = []

test_losses = []

#testing once before we begin
test_loss = test(model, test_dataset, mse)
test_losses.append(test_loss)


#check how model performs on train data once before we begin
train_loss = test(model, train_dataset, mse)
train_losses.append(train_loss)

# We train for num_epochs epochs.
for epoch in range(num_epochs):
    # print('Epoch: __ ' + str(epoch))

    # shuffle the data again in each epoch, so that the network cannot learn the order 'by heart'

    
    #training (and checking in with training)
    for (input,target) in train_dataset:
      # call the train step to compute the loss

    # save the loss by appending it to the list
    train_losses.append(running_average)

    #testing
    test_loss = test(model, test_dataset, mse)
    test_losses.append(test_loss)